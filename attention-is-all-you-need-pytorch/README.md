# Attention is all you need: A Pytorch Implementation

This is a PyTorch implementation of the Transformer model in "[Attention is All You Need](https://arxiv.org/abs/1706.03762)" (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017). 

Github - [https://github.com/jadore801120/attention-is-all-you-need-pytorch]


**Dataset & Training Details**
-	Dataset: WMT 2014 English-German
-	Tokenizer: wordlevel
-	DATASET_SIZE: 30,000
-	TEST_PROPORTION: 0.01
-	Batch size: 48
-	Epochs: 100

**Training Loss**

![image](https://github.com/user-attachments/assets/400e1136-8f71-4363-b8af-8c63ee2e5faf)

**BLEU Score**

![image](https://github.com/user-attachments/assets/e7eb9ae4-3dc7-4596-848c-661e9d78fbff)

**Inference Result**

![image](https://github.com/user-attachments/assets/bd5752bd-0c22-4d68-a966-4931d962eb4c)
